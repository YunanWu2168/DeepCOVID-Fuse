{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HN4DIIBMbs1",
        "outputId": "877e309c-fb6e-44ea-8ec1-f4d845a726da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0513 21:53:47.153772 140382116820800 core.py:46] TF Parameter Server distributed training not available.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pydicom\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from datetime import datetime\n",
        "# from imgaug import augmenters as iaa\n",
        "from scipy import ndimage\n",
        "from math import ceil, floor, log\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import sys\n",
        "import heapq\n",
        "import efficientnet.tfkeras as efn \n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Set the log level of TensorFlow to suppress unnecessary output\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "from tensorflow.python.util import deprecation\n",
        "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "# Import necessary libraries for data preprocessing and modeling\n",
        "# numpy - for numerical operations on arrays\n",
        "# pandas - for data manipulation and analysis\n",
        "# pydicom - for reading DICOM files\n",
        "# os - for interacting with the operating system\n",
        "# matplotlib - for data visualization\n",
        "# collections - for specialized data structures\n",
        "# tqdm - for progress bars during loops\n",
        "# datetime - for working with dates and times\n",
        "# scipy - for scientific computing and image processing\n",
        "# math - for mathematical operations\n",
        "# cv2 - for computer vision tasks\n",
        "# tensorflow - for building and training machine learning models\n",
        "# sys - for system-specific parameters and functions\n",
        "# heapq - for heap-based data structures\n",
        "# efficientnet - a library of efficient neural network architectures\n",
        "# scikit-learn - for machine learning algorithms and model evaluation\n",
        "\n",
        "# Define functions for data preprocessing and modeling here (not shown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7epomblMbs3",
        "outputId": "add5e0b0-f57a-4be7-f8af-0d76957a05eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1489, 99)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preprocess the features  (train + val)\n",
        "tbl = pd.read_csv('# The path of the training clinical features')\n",
        "\n",
        "# combine_id + key\n",
        "tbl['ID'] = tbl.apply(lambda row: str(row.hashed_patient_ir_id) + '_' + row.Hash_Key, axis = 1)\n",
        "tbl = tbl.drop(['hashed_patient_ir_id', 'Hash_Key'], axis = 1)\n",
        "\n",
        "tbl.drop_duplicates(subset=['ID'], keep='first', inplace = True)\n",
        "\n",
        "tbl = tbl.set_index('ID')\n",
        "tbl.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sR-PdwaMbs3",
        "outputId": "a944a7bf-395a-4ffc-afa1-0c4aaaa5f28f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(396, 99)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preprocess the features  (train + val)\n",
        "tbl_val = pd.read_csv('# The path of the validation clinical features')\n",
        "tbl_val.head(5)\n",
        "\n",
        "# combine_id + key\n",
        "tbl_val['ID'] = tbl_val.apply(lambda row: str(row.hashed_patient_ir_id) + '_' + row.Hash_Key, axis = 1)\n",
        "tbl_val = tbl_val.drop(['hashed_patient_ir_id', 'Hash_Key'], axis = 1)\n",
        "tbl_val.drop_duplicates(subset=['ID'], keep='first', inplace = True)\n",
        "\n",
        "tbl_val = tbl_val.set_index('ID')\n",
        "tbl_val.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wnzwFpQMbs4",
        "outputId": "bdf64f0a-a5f2-4b07-bf2c-b9d2f682b866"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(413, 99)"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# preprocess testing dataset \n",
        "tbl_test = pd.read_csv('# The path of the testing clinical features')\n",
        "tbl_test.head(5)\n",
        "\n",
        "# combine_id + key\n",
        "tbl_test['ID'] = tbl_test.apply(lambda row: str(row.hashed_patient_ir_id) + '_' + row.Hash_Key, axis = 1)\n",
        "tbl_test = tbl_test.drop(['hashed_patient_ir_id', 'Hash_Key'], axis = 1)\n",
        "tbl_test.drop_duplicates(subset=['ID'], keep='first', inplace = True)\n",
        "\n",
        "tbl_test = tbl_test.set_index('ID')\n",
        "tbl_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olX32cVuMbs4"
      },
      "outputs": [],
      "source": [
        "def _read_tbl(df, ID, desired_size):    # here the input is \"int\"\n",
        "    try:\n",
        "        row = df.loc[ID,:]\n",
        "    except:\n",
        "        row = np.zeros(desired_size)\n",
        "    return row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bagGvDveMbs4"
      },
      "outputs": [],
      "source": [
        "# Define the size of the input images\n",
        "input_size = (224, 224)\n",
        "\n",
        "# Define the paths to the directories containing the training, validation, and testing CXR features\n",
        "train_images_dir = '# The path of the training CXR features'\n",
        "val_images_dir = '# The path of the validation CXR features'\n",
        "test_images_dir = '# The path of the testing CXR features'\n",
        "\n",
        "# Define the training, validation, and testing clinical features DataFrames\n",
        "train_fs_dir = tbl\n",
        "valid_fs_dir = tbl_val\n",
        "test_fs_dir = tbl_test\n",
        "\n",
        "length = 99\n",
        "\n",
        "class DataGenerator(Sequence):\n",
        "    \"\"\"\n",
        "    Generates data for Keras\n",
        "    \"\"\"\n",
        "    def __init__(self, list_IDs, labels, batch_size=16, fs_dir=train_fs_dir, fs_size=length, augment=False, shuffle=True):\n",
        "        \"\"\"\n",
        "        Initialization\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        self.shuffle = shuffle\n",
        "        self.fs_dir = fs_dir\n",
        "        self.fs_size = fs_size\n",
        "        self.augment = augment\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Denotes the number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Generate one batch of data\n",
        "        \"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X_fs, y = self.__data_generation(list_IDs_temp)\n",
        "\n",
        "        return X_fs, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        \"\"\"\n",
        "        Generates data containing batch_size samples\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        X_fs = np.empty((self.batch_size, self.fs_size))\n",
        "        y = np.empty((self.batch_size), dtype=np.float32)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            ID_split = ID.split('-')\n",
        "            X_fs[i] = _read_tbl(self.fs_dir, ID_split[0], self.fs_size)\n",
        "            y[i] = self.labels.loc[ID].values[0]\n",
        "\n",
        "        return X_fs, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pEzn0JeCMbs4"
      },
      "outputs": [],
      "source": [
        "train_xr = '# The path of the risk level labels'\n",
        "val_xr = '# The path of the risk level labels'\n",
        "df = pd.read_csv(train_xr)\n",
        "df = df.set_index(['ID'])\n",
        "df.index\n",
        "\n",
        "df_val = pd.read_csv(val_xr)\n",
        "df_val = df_val.set_index(['ID'])\n",
        "df_val.index\n",
        "\n",
        "test_xr = '# The path of the risk level labels'\n",
        "\n",
        "df_test = pd.read_csv(test_xr)\n",
        "df_test = df_test.set_index(['ID'])\n",
        "df_test.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP07XubaMbs5"
      },
      "outputs": [],
      "source": [
        "train_seq = DataGenerator(df.index, df, 32,\n",
        "                         fs_dir = train_fs_dir, fs_size = length , shuffle=True)\n",
        "val_seq = DataGenerator(df_val.index, df_val, 32,\n",
        "                         fs_dir = valid_fs_dir, fs_size = length, shuffle=False)\n",
        "\n",
        "test_seq = DataGenerator(df_test.index, df_test, 1,\n",
        "                         fs_dir = test_fs_dir, fs_size = length, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": true,
        "id": "fUJErHtdMbs5"
      },
      "outputs": [],
      "source": [
        "model_1 = tfdf.keras.RandomForestModel(num_trees = 600)\n",
        "model_1.fit(x=train_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aRf4cPzzMbs6"
      },
      "outputs": [],
      "source": [
        "model_1.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_1.evaluate(test_seq, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "O_1hxysBMbs6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = model_1.make_inspector().training_logs()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Logloss (out-of-bag)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.initializers import GlorotNormal\n",
        "\n",
        "initializer = GlorotNormal()\n",
        "fs_in = keras.layers.Input(shape=(length,))  # read the features in \n",
        "feature = keras.layers.Dense(128, kernel_initializer=initializer)(fs_in)\n",
        "feature = keras.activations.relu(feature)\n",
        "feature = keras.layers.Dropout(0.2)(feature)\n",
        "fusion = keras.layers.Dense(64, kernel_initializer=initializer)(feature)\n",
        "fusion = keras.layers.Activation('relu')(fusion)\n",
        "predictions = keras.layers.Dense(3, activation=\"softmax\", name='fusion_last')(fusion)\n",
        "\n",
        "model = keras.models.Model(inputs=fs_in, outputs=predictions)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1t2UaBQbOXzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    \n",
        "    Fold = i\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  optimizer=keras.optimizers.SGD(lr=0.0002, momentum=0.9,nesterov=True),\n",
        "                  metrics=['acc',                       \n",
        "                         keras.metrics.AUC(),\n",
        "                         keras.metrics.Precision(name='precision'),\n",
        "                         keras.metrics.Recall(name='recall')])\n",
        "\n",
        "\n",
        "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "    weightpath = 'Path to save the model.hdf5'.format(Fold)\n",
        "    es = EarlyStopping(monitor='val_acc', \n",
        "                       verbose=1, \n",
        "                       patience=8, \n",
        "                       min_delta=0.000001, \n",
        "                       mode='max')\n",
        "    mc = ModelCheckpoint(weightpath, \n",
        "                         monitor='val_acc', \n",
        "                         verbose=1, \n",
        "                         save_best_only=True, \n",
        "                         mode='max')\n",
        "    rlr = ReduceLROnPlateau(monitor='val_acc',\n",
        "                            mode='max',\n",
        "                            factor=0.1,\n",
        "                            patience=3)\n",
        "    \n",
        "    # class weight\n",
        "    classes = pd.value_counts(df.Level)\n",
        "    print(classes)\n",
        "    # # classes\n",
        "    class_weight = [(classes[3] + classes[1] + classes[2])/classes[1], (classes[3] + classes[1] + classes[2])/classes[2],(classes[3] + classes[1] + classes[2])/classes[3]] \n",
        "    print(class_weight)\n",
        "    \n",
        "    \n",
        "    hist = model.fit_generator(train_seq, epochs=100, verbose=1, max_queue_size=1, \n",
        "                           workers=1, validation_data=val_seq, \n",
        "                           callbacks = [es, mc,rlr], use_multiprocessing=False)  #,  class_weight = class_weight)"
      ],
      "metadata": {
        "id": "2UTmry6-O0SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.savefig('insert_path')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H685KW6YO7fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "MEkGrGWBO8xt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "JaRJLwdkMbs6"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "    for j in range(2):\n",
        "        model.load_weights('The path of the best model.hdf5'.format(i))\n",
        "\n",
        "        Y_pred = model.predict_generator(test_seq)\n",
        "        yy_pred = Y_pred\n",
        "\n",
        "        y_true = test_seq.labels.Label.values\n",
        "\n",
        "        from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score, roc_auc_score, precision_score\n",
        "        y_pred = np.round(Y_pred)\n",
        "        print('Confusion Matrix')\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        target_names = ['COVID-Neg', 'COVID-Pos']\n",
        "        print(cm)\n",
        "\n",
        "        print('i = ', i, 'j = ', j+1)\n",
        "        print(accuracy_score(y_true, y_pred))\n",
        "        print(recall_score(y_true, y_pred))\n",
        "        print(precision_score(y_true, y_pred))\n",
        "        print(f1_score(y_true, y_pred))\n",
        "        print(roc_auc_score(y_true, yy_pred))\n",
        "        print('**********')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Gxi_ClZyMbs7"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, thresholds = roc_curve(y_true, Y_pred)\n",
        "auc = auc(fpr, tpr)\n",
        "plt.figure(1)\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot(fpr, tpr, label='AUC = {:.3f}'.format(auc))\n",
        "plt.xlabel('1-Specificity')\n",
        "plt.ylabel('Sensitivity')\n",
        "plt.title('ROC curve')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR2DECeLMbs7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}